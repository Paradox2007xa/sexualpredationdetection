{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7109c319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a2a798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buses approaching to this place is highly unsafe.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a man was commenting at me.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in a share auto</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was coming out of a club at night with a few...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One of my friends was molested in the crowd. T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I was travelling in metro heading to my colleg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A guy masturbating at the roadside and ogling....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>When I was travelling in red line metro , a gu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Late evening (around 7.30 pm) after shopping i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For a week in the morning when I found be taki...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>It happened in the afternoon in the metro.It w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>some people seemed so friendly,but kept follow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i know of a girl who was groped in the DDA par...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>harassment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a girl was passing and some men started callin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>self explanatory</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A guy usually clicks picture of my friend. It ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>the person took advantage of the crowd while s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>my friend was going to market and one boy comm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>my auto wallah stopped midway and asked us to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>i was walking upstairs on the bridge in dadar ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>it was really bad.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>may be i've been studying 6th then when i was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>It happened with me about 45 years ago. I used...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>i was returning home when a group of men start...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a strange looking man followed the victim from...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>During evening , I went to visit my friend and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>while i was travelling in the bus with my frie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>exposing body parts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>some boys were teasing me when i was going in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>i was home alone with my father. my mother and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>a young girl of 12 years was raped by the step...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>some of the group of boys teases my sister whi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A few years back, I was on a bus form delhi to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>I was harassed and ill treated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Some guys  were stalking a girl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I am 14 year old boy and while returning from ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I had experience while walking in the road.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Someone passed a comment near the Hanuman Temple</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>A man thought I was a prostitute and was offer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pupils of st john primary school in silanga ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>I was walking to my car in a deserted parking ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>By bus driver while driving scooter shouted an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>me and my friend was travelling in the public ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>harassed in the afternoon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Touching, ogling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Chain Snatching, commenting, and ogling in eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>some man whistle my friend when she was going ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Some random guys were clicking pictures of me ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>A young girl was influenced by her friends who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Label\n",
       "0   Buses approaching to this place is highly unsafe.      1\n",
       "1                         a man was commenting at me.      1\n",
       "2                                     in a share auto      0\n",
       "3   I was coming out of a club at night with a few...      1\n",
       "4   One of my friends was molested in the crowd. T...      0\n",
       "5   I was travelling in metro heading to my colleg...      0\n",
       "6   A guy masturbating at the roadside and ogling....      1\n",
       "7   When I was travelling in red line metro , a gu...      0\n",
       "8   Late evening (around 7.30 pm) after shopping i...      0\n",
       "9   For a week in the morning when I found be taki...      1\n",
       "10  It happened in the afternoon in the metro.It w...      1\n",
       "11  some people seemed so friendly,but kept follow...      1\n",
       "12  i know of a girl who was groped in the DDA par...      0\n",
       "13                                         harassment      0\n",
       "14  a girl was passing and some men started callin...      0\n",
       "15                                   self explanatory      0\n",
       "16  A guy usually clicks picture of my friend. It ...      0\n",
       "17  the person took advantage of the crowd while s...      0\n",
       "18  my friend was going to market and one boy comm...      1\n",
       "19  my auto wallah stopped midway and asked us to ...      0\n",
       "20  i was walking upstairs on the bridge in dadar ...      0\n",
       "21                                 it was really bad.      1\n",
       "22  may be i've been studying 6th then when i was ...      0\n",
       "23  It happened with me about 45 years ago. I used...      0\n",
       "24  i was returning home when a group of men start...      1\n",
       "25  a strange looking man followed the victim from...      0\n",
       "26  During evening , I went to visit my friend and...      0\n",
       "27  while i was travelling in the bus with my frie...      1\n",
       "28                                exposing body parts      0\n",
       "29  some boys were teasing me when i was going in ...      1\n",
       "30  i was home alone with my father. my mother and...      0\n",
       "31  a young girl of 12 years was raped by the step...      0\n",
       "32  some of the group of boys teases my sister whi...      1\n",
       "33  A few years back, I was on a bus form delhi to...      0\n",
       "34                     I was harassed and ill treated      1\n",
       "35                    Some guys  were stalking a girl      0\n",
       "36  I am 14 year old boy and while returning from ...      0\n",
       "37        I had experience while walking in the road.      0\n",
       "38   Someone passed a comment near the Hanuman Temple      1\n",
       "39  A man thought I was a prostitute and was offer...      1\n",
       "40  pupils of st john primary school in silanga ar...      0\n",
       "41  I was walking to my car in a deserted parking ...      1\n",
       "42  By bus driver while driving scooter shouted an...      0\n",
       "43  me and my friend was travelling in the public ...      1\n",
       "44                          harassed in the afternoon      1\n",
       "45                                   Touching, ogling      0\n",
       "46  Chain Snatching, commenting, and ogling in eve...      1\n",
       "47  some man whistle my friend when she was going ...      0\n",
       "48  Some random guys were clicking pictures of me ...      0\n",
       "49  A young girl was influenced by her friends who...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dev.csv')\n",
    "data.columns = ['Text', 'Label']\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240d8022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04fc7a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869dc4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for l in data.Label:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860f76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Pos']= pos\n",
    "data['Neg']= neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8236b25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buses approaching to this place is highly unsafe.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a man was commenting at me.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in a share auto</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was coming out of a club at night with a few...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One of my friends was molested in the crowd. T...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label  Pos  Neg\n",
       "0  Buses approaching to this place is highly unsafe.      1    1    0\n",
       "1                        a man was commenting at me.      1    1    0\n",
       "2                                    in a share auto      0    0    1\n",
       "3  I was coming out of a club at night with a few...      1    1    0\n",
       "4  One of my friends was molested in the crowd. T...      0    0    1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a813ca5",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331f425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_nopunct\n",
    "\n",
    "data['Text_Clean'] = data['Text'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce80606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "tokens = [word_tokenize(sen) for sen in data.Text_Clean] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff65f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]  \n",
    "lower_tokens = [lower_token(token) for token in tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54b9e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4ec2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc408d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [remove_stop_words(sen) for sen in lower_tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e16a2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [' '.join(sen) for sen in filtered_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26fa7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text_Final'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f97bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4917508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "973d2f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Label</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buses approaching place highly unsafe</td>\n",
       "      <td>[buses, approaching, place, highly, unsafe]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>man commenting</td>\n",
       "      <td>[man, commenting]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>share auto</td>\n",
       "      <td>[share, auto]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coming club night friends guys came stopped us...</td>\n",
       "      <td>[coming, club, night, friends, guys, came, sto...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text_Final  \\\n",
       "0              buses approaching place highly unsafe   \n",
       "1                                     man commenting   \n",
       "2                                         share auto   \n",
       "3  coming club night friends guys came stopped us...   \n",
       "\n",
       "                                              tokens  Label  Pos  Neg  \n",
       "0        [buses, approaching, place, highly, unsafe]      1    1    0  \n",
       "1                                  [man, commenting]      1    1    0  \n",
       "2                                      [share, auto]      0    0    1  \n",
       "3  [coming, club, night, friends, guys, came, sto...      1    1    0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97f205",
   "metadata": {},
   "source": [
    "# Split data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8cf9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eb1ec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9760 words total, with a vocabulary size of 2638\n",
      "Max sentence length is 116\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7142254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 words total, with a vocabulary size of 556\n",
      "Max sentence length is 48\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0920c1c",
   "metadata": {},
   "source": [
    "# Load Google News Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84581768",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9182cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e6c365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58807a",
   "metadata": {},
   "source": [
    "# Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef58cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c768262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041dbdf",
   "metadata": {},
   "source": [
    "# Tokenize and Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac0a4c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2638 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a55b0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5512b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2639, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25909f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748be391",
   "metadata": {},
   "source": [
    "# Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22eb7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dc64500",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Pos', 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9642407",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25682e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9005250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 50, 300)      791700      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 49, 200)      120200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 48, 200)      180200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 47, 200)      240200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 46, 200)      300200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 45, 200)      360200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 200)         0           ['conv1d[0][0]']                 \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 200)         0           ['conv1d_1[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 200)         0           ['conv1d_2[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_3 (Global  (None, 200)         0           ['conv1d_3[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_4 (Global  (None, 200)         0           ['conv1d_4[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1000)         0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_max_pooling1d_1[0][0]', \n",
      "                                                                  'global_max_pooling1d_2[0][0]', \n",
      "                                                                  'global_max_pooling1d_3[0][0]', \n",
      "                                                                  'global_max_pooling1d_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1000)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          128128      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            258         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,121,086\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 791,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b69f8",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7be2f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 9\n",
    "batch_size = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9529025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "24/24 [==============================] - 2s 48ms/step - loss: 0.6842 - acc: 0.6167 - val_loss: 0.5726 - val_acc: 0.7556\n",
      "Epoch 2/9\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.4696 - acc: 0.7915 - val_loss: 0.5535 - val_acc: 0.7444\n",
      "Epoch 3/9\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 0.3549 - acc: 0.8577 - val_loss: 0.5467 - val_acc: 0.7667\n",
      "Epoch 4/9\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.2274 - acc: 0.9226 - val_loss: 0.5639 - val_acc: 0.8000\n",
      "Epoch 5/9\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 0.1297 - acc: 0.9625 - val_loss: 0.6264 - val_acc: 0.7889\n",
      "Epoch 6/9\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 0.0823 - acc: 0.9725 - val_loss: 0.6769 - val_acc: 0.7778\n",
      "Epoch 7/9\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0687 - acc: 0.9788 - val_loss: 0.7752 - val_acc: 0.7889\n",
      "Epoch 8/9\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.0723 - acc: 0.9788 - val_loss: 0.7330 - val_acc: 0.8000\n",
      "Epoch 9/9\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.0574 - acc: 0.9800 - val_loss: 0.8763 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c9c5d",
   "metadata": {},
   "source": [
    "# Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8e542b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff009605",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f29ad304",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "baa3c80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    61\n",
       "1    38\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53461f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8282828282828283"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34cf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
